# Climate-report
Climate Description! ğŸ˜ğŸ¤



-- ì›ë˜(Lab github Climate) ë‚´ìš© -- 



# ì—°êµ¬ Journal Source !!! (ì€ì°¬ ì‘ì„±ì¤‘ì…ë‹ˆë‹¹.)

í¬ë§· ì œê³µâ˜†: êµìˆ˜ë‹˜ 




## Target 

ICT Express https://www.journals.elsevier.com/ict-express

## í¬ë§· ì°¸ê³  ë…¼ë¬¸(ì€ì°¬, In ICT EXPRESS)
- [A comparative study of LPWAN technologies for large-scale IoT deployment](https://www.sciencedirect.com/science/article/pii/S2405959517302953)


# Title (tentative)

## Classifying Climate Technology in Research Proposals using Pretrained Language Models: A comparative Study  

---

## 1. INTRODUCTION (ì—°êµ¬ ì†Œê°œ)  

**1.1 Deep Learning**  
The Deep Learning refers to stack a lot of Neural Networks Deeply~~~~~~~. 

(ì¤‘ëµ) ..  


Natural Language Processing (NLP) is ~~~.



**1.2. NLP**  

NLP is ì¸ê¸°ê°€ ë§ì•„ì§€ê¸° ì‹œì‘í•œ ê¸°ìˆ ì´ë‹¤. 

Natural Languageë¥¼ íƒ€ê²Ÿìœ¼ë¡œ í•œë‹¤. 

Word Embedding ê¸°ë²•ì„ í†µí•´ì„œ ìì—°ì–´ì˜ ì—°ê´€ê´€ê³„ë¥¼ ë²¡í„°ë¡œ ë‚˜íƒ€ë‚´ê³  ì´ë¥¼ ì´ìš©í•˜ì—¬ ë”¥ëŸ¬ë‹ ë„¤íŠ¸ì›Œí¬ëŠ” ë‹¤ì–‘í•œ ìì—°ì–´ ì²˜ë¦¬ íƒœìŠ¤í¬ì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ~~~ ì¼ë¼ì¼ë¼(ë” ì ìì‹œë‹¤) ~~~~

**1.3. Classification** 

ì „í†µì ìœ¼ë¡œ í–‰í•´ì ¸ ì™”ë˜ ë¨¸ì‹ ëŸ¬ë‹ì˜ í° ë¶„ì•¼ì¸ Classificationì€ ~~~~ ì…ë‹ˆë‹¤. 

ì´ëŠ” ë”¥ëŸ¬ë‹ì—ì„œë„ ë§ˆì°¬ê°€ì§€ë¡œ ë§¤ìš° ë³´í¸ì ì¸ íƒœìŠ¤í¬ì´ë©° ë„ë¦¬ ì“°ì´ê³  ìˆìŠµë‹ˆë‹¤

ìì—°ì–´ ì²˜ë¦¬ì—ì„œë„ ìŠ¤íŒ¸ ë¶„ë¥˜ ì˜í™” ë¦¬ë·° ê¸ì • ë¶€ì • ë¶„ë¥˜ ë“±ì—ì„œ ë”¥ëŸ¬ë‹ ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•œ ìì—°ì–´ ì²˜ë¦¬ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

~~~(ë”ì ê¸°)


**1.4. Language Models**  

**1.5. Objective (ì—°êµ¬ ëª©ì )**  




## 2. METHODS


**2.1. Dataset : Fixed Datasets** 

- feedback in ë©ë¯¸íŒ…) Method - Dataset í¬í•¨ì‹œí‚¤ê¸°(ëŒ€í‘œì ì¸ ì˜ˆ ëª‡ê°œë§Œ) 


**Pre-Processing** 

- 12 Train columns ì¤‘ 4ê°€ì§€ 'ê³¼ì œëª…', 'ìš”ì•½ë¬¸_í•œê¸€í‚¤ì›Œë“œ', 'ìš”ì•½ë¬¸_ì—°êµ¬ëª©í‘œ', 'label' ì‚¬ìš© 
- **Mecab Tokenizer**ë¥¼ í†µí•´ í˜•íƒœì†Œ ë‹¨ìœ„ ë¶„ì ˆ í›„ **ëª…ì‚¬í˜•íƒœë§Œ ì¶”ì¶œí•˜ëŠ” ë°©ì‹**ì„ ìµœì¢… ì 


**2.2. 4 different LMs (BERT, KoBERT, RoBERTa, ELECTRA)**
- BERT 


- KoBERT 


- RoBERTa


- ELECTRA



**2.3. Computation methods + hyperparameters**


- Fixed-Parameter

```(python)
NUM_EPOCHS = 10
VALID_SPLIT = 0.2
MAX_LEN=96

optimizer = tf.keras.optimizers.Adam(3e-5)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')
```


**2.4. Evaluation Method, Public and Private Score** 



## 3. RESULTS
- feedback in ë©ë¯¸íŒ…) ëª¨ë¸ê°„ì˜ ì°¨ì´ë¥¼ êµ¬ë¶„í•˜ë ¤ëŠ”ê±°ë¼ì„œ ë‚˜ì¤‘ì— publicë§Œ ë³´ì—¬ì¤˜ë„ ìƒê´€ ì—†ì„ë“¯

**3.1. BAR GRAPH**



**3.2. TABLE**  

|Filename|Model|Details|Public(F1)|Private(F1)|
|------|------|---|---|---|
|**Model #1**|BERT-base-Multilingual-cased|Fixed-Parameter|0.65340|0.64159|
|**Model #2**|BERT-base-Multilingual-uncased|Fixed-Parameter|0.70660|0.68062|
|**Model #3**|KLUE-BERT-base|Fixed-Parameter|0.74360|0.72007|
|**Model #4**|SKT-KoBERT(cased)|Fixed-Parameter|---|---|
|**Model #5**|KoBERT/monologg|Fixed-Parameter|0.67256|0.66650|
|**Model #6**|RoBERTa-base|Fixed-Parameter|0.59196|0.56519|
|**Model #7**|KLUE-RoBERTa-base|Fixed-Parameter|0.71893|0.69808|
|**Model #8**|KLUE-RoBERTa-large|Fixed-Parameter|---|---|
|**Model #9**|XLM-RoBERTa-large|Fixed-Parameter|0.63074|0.60900|
|**Model #10**|KoELECTRA-small-v3-discriminator|Fixed-Parameter|---|---|  
|**Model #11**|KoELECTRA-base-v3-discriminator|Fixed-Parameter|---|---|  


- ***uncased**: it does not make a difference between english and English.  

**3.3. Performance plot as a function of the number of epochs**


- ì€ì°¬) ê¸°ì¡´ Resultê°€ 10ì—í­ì´ë‹ˆê¹Œ 10 20 30 ë“± descrete í•˜ê²Œ í•˜ê¸° (í•œ ì—í­ì”© êº¾ì€ì„  ê·¸ë˜í”„ë¡œ ë³´ê¸°ëŠ” ë¬´ë¦¬ê°€ ìˆì„ë“¯)  
- **to be late**



## 4. CONCLUSIONS AND DISCUSSION

(Professor: ì´ê²Œ ê¸¸ì–´ì•¼ ì¢‹ì€ ë…¼ë¬¸ì´ë‹¤..)
ã… ã… ã… 


